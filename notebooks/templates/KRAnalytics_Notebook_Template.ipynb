{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a858c826",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '═' (U+2550) (4056986725.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m═══════════════════════════════════════════════════════════════════════════\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '═' (U+2550)\n"
     ]
    }
   ],
   "source": [
    "```xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# {Tier Level}: {Analysis Title} using {Data Source}\n",
    "\n",
    "\n",
    "\n",
    "**Author:** KRAnalytics Team  \n",
    "**Affiliation:** KR-Labs  \n",
    "**Version:** v1.0  \n",
    "**Date:** {YYYY-MM-DD}  \n",
    "**UUID:** {domain-tier-source-###}  \n",
    "**Tier:** {1-6}  \n",
    "**Domain:** {Analytics Model Matrix Domain}\n",
    "\n",
    "## Citation Block\n",
    "\n",
    "To cite this notebook:\n",
    "> KRAnalytics. (2025). {Tier Level}: {Analysis Title} - {Data Source}. \n",
    "> Tier {X} Analytics Framework. https://github.com/KR-Labs/KRAnalytics/\n",
    "\n",
    "## Description\n",
    "\n",
    "**Purpose:** {2-3 sentence description of analytical objective}\n",
    "\n",
    "**Analytics Model Matrix Domain:** {Domain Name}\n",
    "\n",
    "**Data Sources:**\n",
    "- Primary: {Main data source with specific tables/series}\n",
    "- Secondary: {Additional data sources if applicable}\n",
    "- Geographic Coverage: {Geographic scope}\n",
    "\n",
    "**Analytic Methods:**\n",
    "- {Method 1}: {Purpose and description}\n",
    "- {Method 2}: {Purpose and description}\n",
    "- {Method 3}: {Purpose and description}\n",
    "\n",
    "**Business Applications:**\n",
    "1. {Application 1 with specific use case}\n",
    "2. {Application 2 with specific use case}\n",
    "3. {Application 3 with specific use case}\n",
    "\n",
    "**Expected Insights:**\n",
    "- {Insight 1}\n",
    "- {Insight 2}\n",
    "- {Insight 3}\n",
    "\n",
    "**Execution Time:** ~{X-Y} minutes\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Required Notebooks:**\n",
    "- `{Prerequisite_Notebook.ipynb}` - {Description of why it's needed}\n",
    "\n",
    "**Next Steps:**\n",
    "- `{Next_Notebook.ipynb}` - {Description of advanced analysis}\n",
    "\n",
    "**Python Environment:** Python ≥ 3.9\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook demonstrates **Tier {X} {Tier Type} Analytics** for {domain focus}:\n",
    "\n",
    "### Analytical Goals\n",
    "1. {Goal 1 with specific deliverable}\n",
    "2. {Goal 2 with specific deliverable}\n",
    "3. {Goal 3 with specific deliverable}\n",
    "\n",
    "### Business Applications\n",
    "- **{Application Area 1}**: {Specific business value}\n",
    "- **{Application Area 2}**: {Specific business value}\n",
    "- **{Application Area 3}**: {Specific business value}\n",
    "\n",
    "### Data Sources\n",
    "- **Primary**: {Detailed data source information}\n",
    "- **Geographic Levels**: {Coverage details}\n",
    "- **Frequency**: {Update frequency}\n",
    "- **Documentation**: {URL to official documentation}\n",
    "\n",
    "---\n",
    "\n",
    "##  Executive Summary\n",
    "\n",
    "### Key Findings\n",
    "- **{Finding 1}**: {Specific result with numbers}\n",
    "- **{Finding 2}**: {Specific result with numbers}\n",
    "- **{Finding 3}**: {Specific result with numbers}\n",
    "\n",
    "### Business Impact\n",
    "- **{Impact Area 1}**: {Specific business value}\n",
    "- **{Impact Area 2}**: {Specific business value}\n",
    "- **{Impact Area 3}**: {Specific business value}\n",
    "\n",
    "### Methodology\n",
    "- **Data Source**: {Source details}\n",
    "- **Methods**: {Analytical approaches}\n",
    "- **Sample**: {Data characteristics}\n",
    "- **Validation**: {Quality checks}\n",
    "\n",
    "---\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 1. Setup & Library Imports\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# \n",
    "# 1. EXECUTION ENVIRONMENT SETUP\n",
    "# \n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis (add as needed for tier)\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# KRAnalytics utilities\n",
    "from kranalytics.data_utils import load_data_with_fallback, load_sample_data, get_api_key\n",
    "\n",
    "print(\" All libraries imported successfully\")\n",
    "print(f\" pandas version: {pd.__version__}\")\n",
    "print(f\" numpy version: {np.__version__}\")\n",
    "print(f\" plotly version: {px.__version__}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 2. Execution Tracking & Configuration\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# \n",
    "# 2. EXECUTION TRACKING & CONFIGURATION\n",
    "# \n",
    "\n",
    "# Execution tracking setup\n",
    "try:\n",
    "    from kranalytics.khipu_analytics.execution_tracking import setup_notebook_tracking\n",
    "    \n",
    "    metadata = setup_notebook_tracking(\n",
    "        notebook_name=\"{TierX_NotebookName.ipynb}\",\n",
    "        version=\"v1.0\",\n",
    "        seed=42,\n",
    "        save_log=True\n",
    "    )\n",
    "    \n",
    "    print(\" Execution tracking initialized\")\n",
    "    print(f\" Execution ID: {metadata.get('execution_id', 'N/A')}\")\n",
    "    print(f\" Start time: {metadata.get('start_time', 'N/A')}\")\n",
    "    print(f\" Random seed: {metadata.get('seed', 42)}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"  Execution tracking not available (standalone mode)\")\n",
    "    metadata = {}\n",
    "    np.random.seed(42)\n",
    "\n",
    "# Analysis configuration\n",
    "CONFIG = {\n",
    "    'random_seed': 42,\n",
    "    'data_source': '{Primary Data Source}',\n",
    "    'geographic_level': '{state/county/national}',\n",
    "    'analysis_year': 2023,\n",
    "    'sample_size': 1000,  # Adjust based on tier and analysis\n",
    "    'test_size': 0.2,     # For predictive models (Tier 2+)\n",
    "    'confidence_level': 0.95\n",
    "}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key:20}: {value}\")\n",
    "print(\"=\"*80)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 3. Data Loading & Preparation\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# \n",
    "# 3. DATA LOADING & PREPARATION\n",
    "# \n",
    "\n",
    "def load_{dataset}_data(api_key=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Load {dataset} data from {API/Source}.\n",
    "    \n",
    "    This function will be used with load_data_with_fallback to ensure\n",
    "    the notebook works both with and without API keys.\n",
    "    \n",
    "    Args:\n",
    "        api_key: API authentication key\n",
    "        **kwargs: Additional parameters for data loading\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset\n",
    "    \"\"\"\n",
    "    # Implement actual API loading logic here\n",
    "    # This is a placeholder that should be replaced with real API calls\n",
    "    \n",
    "    if api_key:\n",
    "        print(f\" Loading data from {CONFIG['data_source']} API...\")\n",
    "        # Actual API implementation would go here\n",
    "        # For template purposes, we'll return sample data\n",
    "    \n",
    "    # Return sample data for demonstration\n",
    "    return generate_sample_data()\n",
    "\n",
    "def generate_sample_data():\n",
    "    \"\"\"\n",
    "    Generate realistic sample data for {analysis type}.\n",
    "    \n",
    "    This ensures the notebook works without API keys.\n",
    "    \"\"\"\n",
    "    np.random.seed(CONFIG['random_seed'])\n",
    "    \n",
    "    # Generate sample data based on analysis type\n",
    "    # Customize this based on your specific domain\n",
    "    \n",
    "    n_samples = CONFIG['sample_size']\n",
    "    \n",
    "    # Example: Income/demographic data\n",
    "    data = {\n",
    "        'geography': [f\"Area_{i:03d}\" for i in range(1, n_samples + 1)],\n",
    "        'population': np.random.lognormal(10, 1, n_samples).astype(int),\n",
    "        'median_income': np.random.lognormal(11, 0.5, n_samples),\n",
    "        'variable_of_interest': np.random.normal(50, 15, n_samples),\n",
    "        # Add more variables as needed for your analysis\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add derived variables if needed\n",
    "    df['log_income'] = np.log(df['median_income'])\n",
    "    df['income_category'] = pd.cut(df['median_income'], \n",
    "                                   bins=3, \n",
    "                                   labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data with automatic fallback\n",
    "print(\" Loading data...\")\n",
    "\n",
    "df = load_data_with_fallback(\n",
    "    api_loader_func=load_{dataset}_data,\n",
    "    dataset_name='{sample_dataset_name}',\n",
    "    api_key_name='{API_KEY_NAME}',\n",
    "    api_loader_kwargs={'year': CONFIG['analysis_year']}\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" DATA SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\" Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\" Analysis year: {CONFIG['analysis_year']}\")\n",
    "print(f\"  Geographic level: {CONFIG['geographic_level']}\")\n",
    "\n",
    "print(f\"\\n Data preview:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"\\n Descriptive statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(f\"\\n Data quality check:\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "print(\"=\"*80)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 4. Exploratory Data Analysis\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# \n",
    "# 4. EXPLORATORY DATA ANALYSIS\n",
    "# \n",
    "\n",
    "print(\" Creating exploratory visualizations...\")\n",
    "\n",
    "# Create multi-panel EDA dashboard\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        '{Variable 1} Distribution',\n",
    "        '{Variable 1} vs {Variable 2}',\n",
    "        '{Variable 3} by Category',\n",
    "        'Geographic Distribution'\n",
    "    ),\n",
    "    specs=[\n",
    "        [{'type': 'histogram'}, {'type': 'scatter'}],\n",
    "        [{'type': 'box'}, {'type': 'bar'}]\n",
    "    ],\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.12\n",
    ")\n",
    "\n",
    "# 1. Distribution plot\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=df['variable_of_interest'],\n",
    "        nbinsx=30,\n",
    "        marker=dict(color='steelblue', opacity=0.7),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df['median_income'],\n",
    "        y=df['variable_of_interest'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=df['population'],\n",
    "            colorscale='Viridis',\n",
    "            size=8,\n",
    "            opacity=0.6,\n",
    "            colorbar=dict(title='Population', x=1.15)\n",
    "        ),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Box plot by category\n",
    "for i, category in enumerate(['Low', 'Medium', 'High']):\n",
    "    subset = df[df['income_category'] == category]\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=subset['variable_of_interest'],\n",
    "            name=category,\n",
    "            marker=dict(color=['red', 'yellow', 'green'][i]),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. Geographic summary (top 10)\n",
    "top_areas = df.nlargest(10, 'variable_of_interest')\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=top_areas['geography'],\n",
    "        y=top_areas['variable_of_interest'],\n",
    "        marker=dict(color='orange'),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"Value\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Median Income\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Income Category\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Geography\", row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Variable of Interest\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Variable of Interest\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Value\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    title_text=f\"{CONFIG['data_source']} Exploratory Analysis\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlations = df[numeric_cols].corr()\n",
    "\n",
    "print(\"Key correlations with variable of interest:\")\n",
    "target_corr = correlations['variable_of_interest'].sort_values(ascending=False)\n",
    "for var, corr in target_corr.items():\n",
    "    if var != 'variable_of_interest':\n",
    "        print(f\"{var:20}: {corr:6.3f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 5. Core Analysis\n",
    "\n",
    "{Customize this section based on your tier and domain}\n",
    "\n",
    "### Tier 1: Descriptive Statistics\n",
    "### Tier 2: Predictive Modeling  \n",
    "### Tier 3: Time Series Analysis\n",
    "### Tier 4: Clustering Analysis\n",
    "### Tier 5: Advanced Methods\n",
    "### Tier 6: Specialized Techniques\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# \n",
    "# 5. CORE ANALYSIS - {TIER TYPE}\n",
    "# \n",
    "\n",
    "# Customize this section based on your tier:\n",
    "\n",
    "# FOR TIER 1 (DESCRIPTIVE):\n",
    "if metadata.get('tier', 1) == 1:\n",
    "    print(\" Performing Tier 1 Descriptive Analysis...\")\n",
    "    \n",
    "    # Calculate key descriptive statistics\n",
    "    results = {\n",
    "        'mean': df['variable_of_interest'].mean(),\n",
    "        'median': df['variable_of_interest'].median(),\n",
    "        'std': df['variable_of_interest'].std(),\n",
    "        'min': df['variable_of_interest'].min(),\n",
    "        'max': df['variable_of_interest'].max(),\n",
    "        'range': df['variable_of_interest'].max() - df['variable_of_interest'].min(),\n",
    "        'iqr': df['variable_of_interest'].quantile(0.75) - df['variable_of_interest'].quantile(0.25),\n",
    "        'skewness': stats.skew(df['variable_of_interest']),\n",
    "        'kurtosis': stats.kurtosis(df['variable_of_interest'])\n",
    "    }\n",
    "\n",
    "# FOR TIER 2 (PREDICTIVE):\n",
    "elif metadata.get('tier', 1) == 2:\n",
    "    print(\" Performing Tier 2 Predictive Analysis...\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_cols = ['median_income', 'population', 'log_income']  # Customize\n",
    "    X = df[feature_cols]\n",
    "    y = df['variable_of_interest']\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=CONFIG['test_size'], random_state=CONFIG['random_seed']\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(random_state=CONFIG['random_seed'])\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        results[name] = {\n",
    "            'mae': mean_absolute_error(y_test, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'r2': r2_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "# FOR TIER 3+ (TIME SERIES/ADVANCED):\n",
    "else:\n",
    "    print(f\" Performing Tier {metadata.get('tier', 'X')} Advanced Analysis...\")\n",
    "    # Implement advanced analysis specific to your tier\n",
    "    results = {'analysis_type': f\"Tier {metadata.get('tier', 'X')} Analysis\"}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ANALYSIS RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if isinstance(results, dict):\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"\\n{key}:\")\n",
    "            for subkey, subvalue in value.items():\n",
    "                print(f\"  {subkey}: {subvalue:.4f}\")\n",
    "        else:\n",
    "            print(f\"{key:20}: {value:.4f}\" if isinstance(value, (int, float)) else f\"{key:20}: {value}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 6. Advanced Visualizations\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# \n",
    "# 6. ADVANCED VISUALIZATIONS\n",
    "# \n",
    "\n",
    "print(\" Creating advanced visualizations...\")\n",
    "\n",
    "# Domain-specific visualization based on analysis type\n",
    "# Customize these based on your specific domain and tier\n",
    "\n",
    "# Example: Geographic choropleth (if applicable)\n",
    "if 'geography' in df.columns:\n",
    "    fig_map = px.choropleth(\n",
    "        df.head(50),  # Sample for visualization\n",
    "        locations='geography',\n",
    "        color='variable_of_interest',\n",
    "        hover_name='geography',\n",
    "        title=f'{CONFIG[\"data_source\"]} - Geographic Distribution',\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    fig_map.show()\n",
    "\n",
    "# Example: Correlation heatmap\n",
    "if len(numeric_cols) > 3:\n",
    "    fig_corr = px.imshow(\n",
    "        correlations,\n",
    "        title='Variable Correlation Matrix',\n",
    "        color_continuous_scale='RdBu',\n",
    "        aspect='auto'\n",
    "    )\n",
    "    fig_corr.show()\n",
    "\n",
    "# Domain-specific specialized plots\n",
    "# Add plots specific to your analysis domain:\n",
    "\n",
    "# For Income/Inequality: Lorenz curves, Gini visualization\n",
    "# For Employment: Time series, seasonal decomposition\n",
    "# For Crime: Hotspot maps, trend analysis\n",
    "# For Health: Risk matrices, outcome distributions\n",
    "# For Environmental: Burden maps, exposure analysis\n",
    "\n",
    "print(\" Visualizations complete\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 7. Business Insights & Recommendations\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# \n",
    "# 7. BUSINESS INSIGHTS & RECOMMENDATIONS\n",
    "# \n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate insights based on analysis results\n",
    "print(\"\\n KEY INSIGHTS\")\n",
    "\n",
    "if isinstance(results, dict) and 'mean' in results:\n",
    "    # Tier 1 Descriptive Insights\n",
    "    insights = [\n",
    "        f\"1. CENTRAL TENDENCY: Average {CONFIG['data_source'].lower()} value is {results['mean']:.2f} \"\n",
    "        f\"with median {results['median']:.2f}, indicating {'right' if results['mean'] > results['median'] else 'left'} skew.\",\n",
    "        \n",
    "        f\"2. VARIABILITY: Standard deviation of {results['std']:.2f} shows {'high' if results['std'] > results['mean'] * 0.3 else 'moderate'} \"\n",
    "        f\"variation. Range spans {results['range']:.2f} units from {results['min']:.2f} to {results['max']:.2f}.\",\n",
    "        \n",
    "        f\"3. DISTRIBUTION SHAPE: Skewness of {results['skewness']:.2f} indicates \"\n",
    "        f\"{'right-tailed' if results['skewness'] > 0.5 else 'left-tailed' if results['skewness'] < -0.5 else 'symmetric'} distribution. \"\n",
    "        f\"Kurtosis of {results['kurtosis']:.2f} shows {'heavy' if results['kurtosis'] > 1 else 'light'} tails.\"\n",
    "    ]\n",
    "\n",
    "elif isinstance(results, dict) and any('r2' in str(v) for v in results.values()):\n",
    "    # Tier 2 Predictive Insights\n",
    "    best_model = max(results.keys(), key=lambda k: results[k].get('r2', 0))\n",
    "    insights = [\n",
    "        f\"1. MODEL PERFORMANCE: {best_model} achieves highest accuracy with R² = {results[best_model]['r2']:.3f} \"\n",
    "        f\"({results[best_model]['r2']*100:.1f}% variance explained).\",\n",
    "        \n",
    "        f\"2. PREDICTION ACCURACY: Average prediction error of ±{results[best_model]['rmse']:.2f} units \"\n",
    "        f\"with mean absolute error of {results[best_model]['mae']:.2f}.\",\n",
    "        \n",
    "        f\"3. FEATURE IMPORTANCE: {feature_cols[0]} shows strongest correlation \"\n",
    "        f\"(r={target_corr.iloc[1]:.3f}) with target variable.\"\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    # Generic insights\n",
    "    insights = [\n",
    "        f\"1. DATA OVERVIEW: Analysis covers {len(df):,} {CONFIG['geographic_level']} areas \"\n",
    "        f\"from {CONFIG['data_source']} for {CONFIG['analysis_year']}.\",\n",
    "        \n",
    "        f\"2. QUALITY ASSESSMENT: Dataset shows {df.isnull().sum().sum()} missing values \"\n",
    "        f\"and {df.duplicated().sum()} duplicates, indicating {'high' if df.isnull().sum().sum() == 0 else 'moderate'} quality.\",\n",
    "        \n",
    "        f\"3. GEOGRAPHIC COVERAGE: Analysis spans {df['geography'].nunique() if 'geography' in df.columns else 'multiple'} \"\n",
    "        f\"geographic areas with comprehensive {CONFIG['geographic_level']}-level detail.\"\n",
    "    ]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"\\n {insight}\")\n",
    "\n",
    "# Strategic recommendations\n",
    "print(\"\\n\\n STRATEGIC RECOMMENDATIONS\")\n",
    "\n",
    "recommendations = [\n",
    "    f\"1. SHORT-TERM (0-6 months): Focus on top-performing areas identified in analysis. \"\n",
    "    f\"Target the {len(df.nlargest(10, 'variable_of_interest'))} highest-value areas for immediate intervention.\",\n",
    "    \n",
    "    f\"2. MEDIUM-TERM (6-18 months): Develop targeted strategies for underperforming areas. \"\n",
    "    f\"Address the {len(df.nsmallest(10, 'variable_of_interest'))} lowest-value areas with tailored programs.\",\n",
    "    \n",
    "    f\"3. LONG-TERM (18+ months): Implement systematic monitoring using this analytical framework. \"\n",
    "    f\"Update analysis quarterly with new {CONFIG['data_source']} data.\",\n",
    "    \n",
    "    f\"4. RESOURCE ALLOCATION: Prioritize investments based on analysis findings. \"\n",
    "    f\"Focus 60% of resources on high-impact areas identified through modeling.\",\n",
    "    \n",
    "    f\"5. PERFORMANCE MONITORING: Establish KPIs based on key variables. \"\n",
    "    f\"Track {len(numeric_cols)} key metrics monthly for early warning indicators.\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"\\n {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 8. Conclusion & Next Steps\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# \n",
    "# 8. CONCLUSION & NEXT STEPS\n",
    "# \n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate conclusion based on analysis type\n",
    "if metadata.get('tier', 1) == 1:\n",
    "    analysis_type = \"descriptive analysis\"\n",
    "    key_value = f\"comprehensive statistical overview of {len(df):,} {CONFIG['geographic_level']} areas\"\n",
    "elif metadata.get('tier', 1) == 2:\n",
    "    analysis_type = \"predictive modeling\"\n",
    "    key_value = f\"{results[best_model]['r2']*100:.1f}% prediction accuracy\"\n",
    "else:\n",
    "    analysis_type = f\"Tier {metadata.get('tier', 'X')} analysis\"\n",
    "    key_value = f\"advanced insights across {len(df):,} observations\"\n",
    "\n",
    "print(\n",
    "    f\"\\nThis {analysis_type} of {CONFIG['data_source']} data provides {key_value}, \"\n",
    "    f\"enabling evidence-based decision-making for {CONFIG['analysis_year']}. \"\n",
    "    f\"The framework demonstrates robust analytical capabilities with \"\n",
    "    f\"{'high data quality' if df.isnull().sum().sum() == 0 else 'acceptable data quality'} \"\n",
    "    f\"and comprehensive geographic coverage.\\n\\n\"\n",
    "    \n",
    "    f\"Key business value:\\n\"\n",
    "    f\"- Strategic insights for {CONFIG['geographic_level']}-level planning\\n\"\n",
    "    f\"- Evidence-based resource allocation recommendations\\n\"\n",
    "    f\"- Systematic framework for ongoing monitoring\\n\"\n",
    "    f\"- Scalable methodology for broader application\\n\\n\"\n",
    "    \n",
    "    f\"Production deployment: Quarterly data updates, automated monitoring, \"\n",
    "    f\"integration with operational dashboards.\\n\"\n",
    ")\n",
    "\n",
    "print(\"\\n NEXT STEPS & RELATED ANALYSES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Next steps based on tier progression\n",
    "next_steps = [\n",
    "    (f\"Tier{metadata.get('tier', 1)+1}_{CONFIG['data_source']}_Advanced.ipynb\", \n",
    "     f\"Advanced analysis building on these {analysis_type} foundations\"),\n",
    "    \n",
    "    (\"Geographic_Expansion_Analysis.ipynb\", \n",
    "     f\"Extend analysis to additional {CONFIG['geographic_level']} areas\"),\n",
    "    \n",
    "    (\"Temporal_Trend_Analysis.ipynb\", \n",
    "     f\"Multi-year trend analysis using historical {CONFIG['data_source']} data\"),\n",
    "    \n",
    "    (\"Cross_Domain_Integration.ipynb\", \n",
    "     f\"Integrate with other Analytics Model Matrix domains\")\n",
    "]\n",
    "\n",
    "for notebook, description in next_steps:\n",
    "    print(f\"\\n• {notebook}\")\n",
    "    print(f\"  {description}\")\n",
    "\n",
    "# Finalize execution tracking\n",
    "try:\n",
    "    from kranalytics.khipu_analytics.execution_tracking import finalize_notebook_tracking\n",
    "    \n",
    "    final_metadata = finalize_notebook_tracking(\n",
    "        metadata,\n",
    "        results={'analysis_type': analysis_type, 'data_points': len(df)},\n",
    "        errors=[]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Execution completed successfully\")\n",
    "    print(f\"⏱  Total duration: {final_metadata.get('duration_formatted', 'N/A')}\")\n",
    "    print(f\" Execution log: {final_metadata.get('log_file', 'Not saved')}\")\n",
    "    \n",
    "except:\n",
    "    print(f\"\\n Analysis completed successfully\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  Analysis complete. Notebook ready for production deployment.\")\n",
    "print(\"=\"*80)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Appendix: Technical Notes\n",
    "\n",
    "### Data Sources\n",
    "- **Primary**: {Detailed data source documentation}\n",
    "- **Quality**: {Data quality assessment}\n",
    "- **Limitations**: {Known limitations and caveats}\n",
    "\n",
    "### Methodology\n",
    "- **Analytical Framework**: {Statistical/ML methods used}\n",
    "- **Validation**: {How results were validated}\n",
    "- **Assumptions**: {Key assumptions made}\n",
    "\n",
    "### Reproducibility\n",
    "- **Random Seed**: {seed value} for consistent results\n",
    "- **Environment**: Python {version}, key package versions\n",
    "- **Execution Time**: ~{X} minutes on standard hardware\n",
    "\n",
    "### References\n",
    "1. {Primary methodology reference}\n",
    "2. {Data source documentation}\n",
    "3. {Relevant academic papers}\n",
    "\n",
    "---\n",
    "\n",
    "**Version History:**\n",
    "- v1.0: Initial analysis framework\n",
    "- v1.1: Enhanced visualizations (planned)\n",
    "- v2.0: Integration with live APIs (planned)\n",
    "\n",
    "**Contact:** info@krlabs.dev for questions about this analysis.\n",
    "\n",
    "---\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}